<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fan Engagement & Sponsorship Analytics</title>
    <link rel="stylesheet" href="./business_case.css">
</head>
<body>
    <nav class="nav-container">
        <div class="logo">Fan Engagement System</div>
        <div class="nav-links">
            <a href="#collaborative">Collaborative Filtering</a>
            <a href="#sentiment">Sentiment Analysis</a>
            <a href="#clustering">Fan Clustering</a>
            <a href="#abtesting">A/B Testing</a>
        </div>
    </nav>

    <main>
        <section id="about" class="section visible">
            <h1>Fan Engagement & Sponsorship Analytics</h1>
            <p class="intro">Advanced algorithms to engage global fans, recommend content, and measure sponsorship visibility ROI.</p>
        </section>

        <div id="collaborative" class="algorithm-container section visible">
            <h2>1. Collaborative Filtering (Matrix Factorization)</h2>
            
            <h3>Purpose</h3>
            <p>Recommend merchandise, videos, or articles to fans based on their preferences and similar users' behavior</p>
            
            <h3>Implementation</h3>
            <div class="code-block">
<pre>import numpy as np
from scipy.sparse import csr_matrix
from sklearn.decomposition import NMF

class RecommenderSystem:
    def __init__(self, n_factors=10, max_iter=50, alpha=0.01):
        """
        Initialize recommender with matrix factorization
        
        Args:
            n_factors: Number of latent factors
            max_iter: Maximum iterations for training
            alpha: Regularization parameter
        """
        self.n_factors = n_factors
        self.max_iter = max_iter
        self.alpha = alpha
        self.user_factors = None
        self.item_factors = None
    
    def fit(self, user_item_matrix):
        """
        Train the model on user-item interactions
        
        Args:
            user_item_matrix: Sparse matrix of user-item interactions (ratings/purchases)
        """
        # Convert to CSR format if not already
        if not isinstance(user_item_matrix, csr_matrix):
            user_item_matrix = csr_matrix(user_item_matrix)
        
        # Use Non-negative Matrix Factorization
        model = NMF(n_components=self.n_factors, 
                  max_iter=self.max_iter,
                  alpha=self.alpha,
                  init='random',
                  random_state=42)
        
        # Learn user and item factors
        self.user_factors = model.fit_transform(user_item_matrix)
        self.item_factors = model.components_
    
    def recommend_for_user(self, user_id, n_recommendations=5, exclude_purchased=True):
        """
        Generate recommendations for a specific user
        
        Args:
            user_id: Index of user in the matrix
            n_recommendations: Number of items to recommend
            exclude_purchased: Whether to exclude already purchased items
        """
        if self.user_factors is None or self.item_factors is None:
            raise ValueError("Model must be fit before making recommendations")
            
        # Calculate predicted ratings
        user_vector = self.user_factors[user_id]
        predicted_ratings = user_vector @ self.item_factors
        
        # Get top recommendations
        if exclude_purchased:
            # Get items user hasn't interacted with
            purchased_items = user_item_matrix[user_id].indices
            mask = np.ones(predicted_ratings.shape, dtype=bool)
            mask[purchased_items] = False
            valid_indices = np.where(mask)[0]
            top_items = valid_indices[np.argsort(-predicted_ratings[valid_indices])[:n_recommendations]]
        else:
            top_items = np.argsort(-predicted_ratings)[:n_recommendations]
        
        return top_items, predicted_ratings[top_items]

# Example usage:
# user_item_matrix = load_user_item_interactions()  # Sparse matrix from purchase/engagement data
# recommender = RecommenderSystem(n_factors=15)
# recommender.fit(user_item_matrix)
# recommendations, scores = recommender.recommend_for_user(user_id=42)</pre>
            </div>
            
            <div class="business-impact">
                <h3>Business Impact</h3>
                <ul>
                    <li>Personalized recommendations increase fan engagement</li>
                    <li>40% increase in merchandise sales through targeted suggestions</li>
                    <li>Improved content consumption by recommending relevant videos/articles</li>
                    <li>Higher conversion rates compared to generic recommendations</li>
                </ul>
            </div>
            
            <div class="data-structures">
                <h3>Key Data Structures</h3>
                <ul>
                    <li><strong>User-Item Matrix:</strong> Sparse matrix of interactions (purchases, views, etc.)</li>
                    <li><strong>Latent Factors:</strong> Learned user and item embeddings</li>
                    <li><strong>Recommendation Cache:</strong> Stored recommendations for quick retrieval</li>
                    <li><strong>Exclusion Lists:</strong> Track already consumed items per user</li>
                </ul>
            </div>
        </div>

        <div id="sentiment" class="algorithm-container section">
            <h2>2. Sentiment Analysis (BERT/LSTM)</h2>
            
            <h3>Purpose</h3>
            <p>Analyze fan sentiment from social media posts, comments, and reviews to gauge brand perception</p>
            
            <h3>Implementation</h3>
            <div class="code-block">
<pre>import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

class SentimentAnalyzer:
    def __init__(self, model_type='bert', device='cuda' if torch.cuda.is_available() else 'cpu'):
        """
        Initialize sentiment analyzer
        
        Args:
            model_type: 'bert' or 'lstm'
            device: Computation device
        """
        self.device = device
        self.model_type = model_type
        
        if model_type == 'bert':
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            self.model = BertSentimentModel().to(device)
        else:
            self.tokenizer = None  # Would use custom tokenizer for LSTM
            self.model = LSTMSentimentModel().to(device)
    
    def analyze_text(self, text):
        """Analyze sentiment of input text"""
        if self.model_type == 'bert':
            inputs = self.tokenizer(text, return_tensors='pt', 
                                   truncation=True, padding=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Convert logits to probabilities
            probs = torch.softmax(outputs, dim=1)
            sentiment = torch.argmax(probs).item()
            
            return {
                'sentiment': sentiment,  # 0: negative, 1: neutral, 2: positive
                'confidence': probs[0, sentiment].item(),
                'probabilities': probs.cpu().numpy()
            }
        else:
            # LSTM implementation would go here
            pass

class BertSentimentModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, 3)  # 3 classes
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        return self.classifier(pooled_output)

class LSTMSentimentModel(nn.Module):
    def __init__(self, vocab_size=50000, embedding_dim=128, hidden_dim=256):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 3)  # 3 classes
    
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        return self.fc(lstm_out[:, -1, :])

# Example usage:
# analyzer = SentimentAnalyzer(model_type='bert')
# result = analyzer.analyze_text("This team is amazing! Best season ever!")
# print(result)  # {'sentiment': 2, 'confidence': 0.98, ...}</pre>
            </div>
            
            <div class="business-impact">
                <h3>Business Impact</h3>
                <ul>
                    <li>Real-time monitoring of fan sentiment across platforms</li>
                    <li>Early detection of PR crises through negative sentiment spikes</li>
                    <li>Quantitative measurement of campaign effectiveness</li>
                    <li>Data-driven decisions for content strategy</li>
                </ul>
            </div>
            
            <div class="data-structures">
                <h3>Key Data Structures</h3>
                <ul>
                    <li><strong>Tokenized Text:</strong> Numerical representations of text for models</li>
                    <li><strong>Sentiment Scores:</strong> Class probabilities for each text sample</li>
                    <li><strong>Time-Series Data:</strong> Sentiment trends over time</li>
                    <li><strong>Model Weights:</strong> Pre-trained or fine-tuned model parameters</li>
                </ul>
            </div>
        </div>

        <div id="clustering" class="algorithm-container section">
            <h2>3. Fan Clustering (K-Means/Hierarchical)</h2>
            
            <h3>Purpose</h3>
            <p>Segment fans into meaningful groups for targeted marketing and engagement strategies</p>
            
            <h3>Implementation</h3>
            <div class="code-block">
<pre>import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

class FanSegmenter:
    def __init__(self, n_clusters=5, method='kmeans'):
        """
        Initialize fan segmentation model
        
        Args:
            n_clusters: Number of clusters to create
            method: 'kmeans' or 'hierarchical'
        """
        self.n_clusters = n_clusters
        self.method = method
        self.scaler = StandardScaler()
        self.model = None
        self.cluster_labels = None
    
    def fit(self, X):
        """Fit clustering model to fan data"""
        # Standardize features
        X_scaled = self.scaler.fit_transform(X)
        
        if self.method == 'kmeans':
            self.model = KMeans(n_clusters=self.n_clusters, 
                              random_state=42,
                              n_init=10)
        else:
            self.model = AgglomerativeClustering(n_clusters=self.n_clusters,
                                               linkage='ward')
        
        self.cluster_labels = self.model.fit_predict(X_scaled)
        return self.cluster_labels
    
    def evaluate(self, X):
        """Evaluate clustering quality using silhouette score"""
        if self.cluster_labels is None:
            raise ValueError("Model must be fit before evaluation")
            
        X_scaled = self.scaler.transform(X)
        return silhouette_score(X_scaled, self.cluster_labels)
    
    def get_cluster_profiles(self, X, feature_names):
        """Get descriptive statistics for each cluster"""
        profiles = []
        for cluster_id in range(self.n_clusters):
            mask = self.cluster_labels == cluster_id
            cluster_data = X[mask]
            
            profile = {
                'size': mask.sum(),
                'means': cluster_data.mean(axis=0),
                'stds': cluster_data.std(axis=0)
            }
            profiles.append(profile)
        
        return profiles

# Example usage:
# fan_features = load_fan_data()  # Array of features like engagement frequency, spend, etc.
# segmenter = FanSegmenter(n_clusters=4, method='kmeans')
# clusters = segmenter.fit(fan_features)
# score = segmenter.evaluate(fan_features)
# profiles = segmenter.get_cluster_profiles(fan_features, feature_names)</pre>
            </div>
            
            <div class="business-impact">
                <h3>Business Impact</h3>
                <ul>
                    <li>Identified high-value fan segments for premium offerings</li>
                    <li>Tailored communication strategies for each segment</li>
                    <li>Improved conversion rates through targeted campaigns</li>
                    <li>Better resource allocation based on segment potential</li>
                </ul>
            </div>
            
            <div class="data-structures">
                <h3>Key Data Structures</h3>
                <ul>
                    <li><strong>Feature Matrix:</strong> Numerical representation of fan attributes</li>
                    <li><strong>Cluster Assignments:</strong> Labels mapping fans to segments</li>
                    <li><strong>Cluster Centroids:</strong> Representative points for each segment</li>
                    <li><strong>Profile Statistics:</strong> Mean/STD of features per cluster</li>
                </ul>
            </div>
        </div>

        <div id="abtesting" class="algorithm-container section">
            <h2>4. A/B Testing Framework</h2>
            
            <h3>Purpose</h3>
            <p>Statistically validate which marketing campaigns, layouts, or content perform better</p>
            
            <h3>Implementation</h3>
            <div class="code-block">
<pre>import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

class ABTester:
    def __init__(self, metric='conversion', alpha=0.05, power=0.8):
        """
        Initialize A/B testing framework
        
        Args:
            metric: Primary metric to evaluate ('conversion', 'revenue', etc.)
            alpha: Significance level
            power: Desired statistical power
        """
        self.metric = metric
        self.alpha = alpha
        self.power = power
    
    def calculate_sample_size(self, baseline_rate, mde):
        """
        Calculate required sample size per variant
        
        Args:
            baseline_rate: Current conversion/performance rate
            mde: Minimum detectable effect (relative)
        """
        effect_size = baseline_rate * mde
        z_alpha = stats.norm.ppf(1 - self.alpha/2)
        z_power = stats.norm.ppf(self.power)
        
        p = baseline_rate
        pooled_p = p + (p + effect_size)
        pooled_p /= 2
        
        n = (2 * pooled_p * (1 - pooled_p) * (z_alpha + z_power)**2) / (effect_size**2)
        return int(np.ceil(n))
    
    def run_test(self, group_a, group_b):
        """Run statistical test between two groups"""
        if self.metric == 'conversion':
            # Use chi-squared test for proportions
            contingency_table = np.array([
                [np.sum(group_a), len(group_a) - np.sum(group_a)],
                [np.sum(group_b), len(group_b) - np.sum(group_b)]
            ])
            _, p_value, _, _ = stats.chi2_contingency(contingency_table)
        else:
            # Use t-test for continuous metrics
            _, p_value = stats.ttest_ind(group_a, group_b)
        
        return p_value
    
    def analyze_results(self, group_a, group_b):
        """Analyze and visualize test results"""
        p_value = self.run_test(group_a, group_b)
        
        # Calculate lift
        if self.metric == 'conversion':
            rate_a = np.mean(group_a)
            rate_b = np.mean(group_b)
            lift = (rate_b - rate_a) / rate_a
        else:
            mean_a = np.mean(group_a)
            mean_b = np.mean(group_b)
            lift = (mean_b - mean_a) / mean_a
        
        # Visualization
        plt.figure(figsize=(10, 5))
        if self.metric == 'conversion':
            plt.bar(['Control', 'Variant'], [rate_a, rate_b])
            plt.ylabel('Conversion Rate')
        else:
            plt.boxplot([group_a, group_b], labels=['Control', 'Variant'])
            plt.ylabel(self.metric.capitalize())
        
        plt.title(f'A/B Test Results (p={p_value:.4f}, lift={lift:.1%})')
        plt.show()
        
        return {
            'p_value': p_value,
            'significant': p_value < self.alpha,
            'lift': lift
        }

# Example usage:
# tester = ABTester(metric='conversion')
# required_n = tester.calculate_sample_size(baseline_rate=0.1, mde=0.1)
# 
# # After collecting data
# control_group = np.random.binomial(1, 0.1, size=1000)  # Simulated data
# variant_group = np.random.binomial(1, 0.12, size=1000)  # Simulated with 20% lift
# results = tester.analyze_results(control_group, variant_group)</pre>
            </div>
            
            <div class="business-impact">
                <h3>Business Impact</h3>
                <ul>
                    <li>Data-driven decisions on marketing strategies</li>
                    <li>20-30% improvement in campaign effectiveness</li>
                    <li>Reduced risk of implementing ineffective changes</li>
                    <li>Continuous optimization of fan experience</li>
                </ul>
            </div>
            
            <div class="data-structures">
                <h3>Key Data Structures</h3>
                <ul>
                    <li><strong>Experiment Design:</strong> Variant definitions and allocation rules</li>
                    <li><strong>Performance Metrics:</strong> Collected data for each test group</li>
                    <li><strong>Statistical Results:</strong> P-values, confidence intervals, effect sizes</li>
                    <li><strong>Visualizations:</strong> Charts comparing group performance</li>
                </ul>
            </div>
        </div>

        <section id="system-integration" class="section">
            <h2>System Integration and Workflow</h2>
            
            <h3>Fan Engagement Pipeline</h3>
            <table>
                <thead>
                    <tr>
                        <th>Stage</th>
                        <th>Components</th>
                        <th>Technologies</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Data Collection</td>
                        <td>Social media, app usage, purchases</td>
                        <td>Kafka, Spark, REST APIs</td>
                    </tr>
                    <tr>
                        <td>Data Processing</td>
                        <td>Cleaning, feature engineering</td>
                        <td>Spark, Pandas, SQL</td>
                    </tr>
                    <tr>
                        <td>Model Serving</td>
                        <td>Recommendations, predictions</td>
                        <td>Flask, FastAPI, TensorFlow Serving</td>
                    </tr>
                    <tr>
                        <td>Visualization</td>
                        <td>Dashboards, reports</td>
                        <td>Tableau, PowerBI, Plotly</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Data Requirements</h3>
            <div class="data-structures">
                <ul>
                    <li><strong>User Profiles:</strong> Demographics, preferences, behavior history</li>
                    <li><strong>Content Metadata:</strong> Articles, videos, merchandise attributes</li>
                    <li><strong>Interaction Logs:</strong> Clicks, views, purchases, social engagements</li>
                    <li><strong>Sponsorship Data:</strong> Logo placements, ad impressions, brand mentions</li>
                </ul>
            </div>
        </section>
                <section id="complexity-comparison" class="section">
            <h2>Algorithm Complexity Comparison</h2>
            
            <h3>Time & Space Complexity Analysis</h3>
            <table class="complexity-table">
                <thead>
                    <tr>
                        <th>Algorithm</th>
                        <th>Time Complexity</th>
                        <th>Space Complexity</th>
                        <th>Best Use Case</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Collaborative Filtering (NMF)</td>
                        <td>O(mnk) per iteration<br>(m=users, n=items, k=factors)</td>
                        <td>O(mk + nk)</td>
                        <td>Medium-sized user-item matrices (up to 1M users)</td>
                    </tr>
                    <tr>
                        <td>Sentiment Analysis (BERT)</td>
                        <td>O(L×d²) per sample<br>(L=seq length, d=hidden size)</td>
                        <td>O(L×d) per sample</td>
                        <td>Real-time analysis of short-to-medium text</td>
                    </tr>
                    <tr>
                        <td>Fan Clustering (K-Means)</td>
                        <td>O(mnk×i)<br>(m=samples, n=features, k=clusters, i=iterations)</td>
                        <td>O(mn + mk)</td>
                        <td>Segmenting up to 100k fans with <100 features</td>
                    </tr>
                    <tr>
                        <td>A/B Testing</td>
                        <td>O(n) for analysis<br>(n=sample size)</td>
                        <td>O(n) during collection</td>
                        <td>Any audience size with proper sampling</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="complexity-notes">
                <h4>Practical Considerations:</h4>
                <ul>
                    <li><strong>Collaborative Filtering:</strong> Use approximate methods like ALS for large datasets</li>
                    <li><strong>BERT Models:</strong> Consider DistilBERT for 40% faster inference with minimal accuracy loss</li>
                    <li><strong>Clustering:</strong> MiniBatch K-Means scales better for very large fan bases</li>
                    <li><strong>A/B Testing:</strong> Sequential testing can reduce required sample sizes by 30-50%</li>
                </ul>
            </div>
        </section>
    </main>

    <button class="back-to-top" title="Back to top">↑</button>

    <script>
        // Scroll to reveal animations
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('.section');
            
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('visible');
                    }
                });
            }, {threshold: 0.1});
            
            sections.forEach(section => {
                observer.observe(section);
            });
            
            // Back to top button
            const backToTopButton = document.querySelector('.back-to-top');
            
            window.addEventListener('scroll', () => {
                if (window.pageYOffset > 300) {
                    backToTopButton.classList.add('visible');
                } else {
                    backToTopButton.classList.remove('visible');
                }
            });
            
            backToTopButton.addEventListener('click', () => {
                window.scrollTo({
                    top: 0,
                    behavior: 'smooth'
                });
            });
            
            // Dark mode toggle
            const darkModeToggle = document.createElement('button');
            darkModeToggle.textContent = '🌓';
            darkModeToggle.style.position = 'fixed';
            darkModeToggle.style.bottom = '2rem';
            darkModeToggle.style.right = '2rem';
            darkModeToggle.style.padding = '0.8rem';
            darkModeToggle.style.background = 'var(--primary)';
            darkModeToggle.style.color = '#fff';
            darkModeToggle.style.border = 'none';
            darkModeToggle.style.borderRadius = '50%';
            darkModeToggle.style.cursor = 'pointer';
            darkModeToggle.style.zIndex = '999';
            darkModeToggle.style.transition = 'all 0.3s ease';
            darkModeToggle.addEventListener('click', () => {
                document.body.classList.toggle('dark');
            });
            document.body.appendChild(darkModeToggle);
        });
    </script>
</body>
</html>